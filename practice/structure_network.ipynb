{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfc2a6e",
   "metadata": {},
   "source": [
    "# 神经网络的搭建\n",
    "\n",
    "深度学习案例的4个步骤：\n",
    "1. 准备数据\n",
    "2. 搭建神经网络\n",
    "3. 模型训练\n",
    "4. 模型测试\n",
    "\n",
    "搭建神经网络的流程\n",
    "1. 定义一个类，继承自`nn.Module`\n",
    "2. 在类的初始化方法__init__()中定义神经网络的层\n",
    "3. 实现`forward`方法，定义前向传播的计算过程\n",
    "\n",
    "接下来我们来构建如下图所示的神经网络模型：\n",
    "\n",
    "<img src=\"./docs/02-神经网络基础/assets/image-20220313133601678.png\" alt=\"image-20220313133601678\" style=\"zoom:50%;\" />\n",
    "\n",
    "**编码设计如下：**\n",
    "\n",
    "- 第1个隐藏层：权重初始化采用标准化的xavier初始化 激活函数使用sigmoid\n",
    "- 第2个隐藏层：权重初始化采用标准化的HeKaiming初始化 激活函数采用relu\n",
    "- out输出层线性层 假若多分类，采用softmax做数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "搭建神经网络\n",
    "\n",
    "下载torchsummary，用来计算模型参数，查看模型结构\n",
    "pip install torchsummary\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "# 1. 搭建神经网络，自定义继承nn.Module\n",
    "class ModuleDemo(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 1.1 初始化父类成员\n",
    "        super().__init__()\n",
    "        # 1.2 搭建神经网络 --> 隐藏层 + 输出层\n",
    "        # 隐藏层1：输入特征数3，输出特征数3\n",
    "        self.linear1 = nn.Linear(3, 3)\n",
    "        # 隐藏层2：输入特征数3，输出特征数2\n",
    "        self.linear2 = nn.Linear(3, 2)\n",
    "        # 输出层：输入特征数2，输出特征数2\n",
    "        self.output = nn.Linear(2, 2)\n",
    "\n",
    "        # 1.3 对隐藏层进行参数初始化\n",
    "        # 隐藏层1\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "\n",
    "        # 隐藏层2\n",
    "        nn.init.kaiming_normal_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "\n",
    "        # 输出层\n",
    "\n",
    "    # 2.前向传播\n",
    "    def forward(self, x):\n",
    "        # 2.1 第一层，隐藏层计算：加权求和 + 激活函数sigmoid\n",
    "        # 分解版写法\n",
    "        # x = self.linear1(x)  # 加权求和\n",
    "        # x = torch.sigmoid(x)  # 激活函数\n",
    "\n",
    "        # 合并版写法\n",
    "        x = torch.sigmoid(self.linear1(x))  # 加权求和 + 激活函数\n",
    "\n",
    "        # 2.2 第二层，隐藏层计算：加权求和 + 激活函数relu\n",
    "        x = torch.relu(self.linear2(x)) \n",
    "\n",
    "        # 2.3 第三层，输出层计算：加权求和\n",
    "        # dim=-1 表示对最后一个维度进行softmax操作\n",
    "        x = torch.softmax(self.output(x), dim=-1)\n",
    "\n",
    "        # 2.4 返回预测值\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aa465e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_module: ModuleDemo(\n",
      "  (linear1): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (output): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "data: tensor([[-0.5950, -0.0918, -0.1226],\n",
      "        [-0.8373,  0.9182, -0.2404],\n",
      "        [-0.2893, -0.8418, -1.7544],\n",
      "        [ 0.2223,  1.3177,  1.6061],\n",
      "        [-0.1040, -0.5004,  1.6244]])\n",
      "output: tensor([[0.6701, 0.3299],\n",
      "        [0.6780, 0.3220],\n",
      "        [0.6712, 0.3288],\n",
      "        [0.6731, 0.3269],\n",
      "        [0.6584, 0.3416]], grad_fn=<SoftmaxBackward0>)\n",
      "output shape: torch.Size([5, 2])\n",
      "output required_grad: True\n",
      "==============计算模型参数==============\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 5, 3]              12\n",
      "            Linear-2                 [-1, 5, 2]               8\n",
      "            Linear-3                 [-1, 5, 2]               6\n",
      "================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "==============查看模型参数==============\n",
      "参数名: linear1.weight\n",
      "参数值: Parameter containing:\n",
      "tensor([[-1.0679,  0.1725, -0.2811],\n",
      "        [ 0.9581,  0.7407,  0.0044],\n",
      "        [ 0.1931, -1.3819, -0.3266]], requires_grad=True)\n",
      "\n",
      "参数名: linear1.bias\n",
      "参数值: Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "\n",
      "参数名: linear2.weight\n",
      "参数值: Parameter containing:\n",
      "tensor([[-1.5157, -0.5103, -0.4574],\n",
      "        [ 1.0894,  1.1049,  0.2616]], requires_grad=True)\n",
      "\n",
      "参数名: linear2.bias\n",
      "参数值: Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "\n",
      "参数名: output.weight\n",
      "参数值: Parameter containing:\n",
      "tensor([[0.6588, 0.6444],\n",
      "        [0.4595, 0.4226]], requires_grad=True)\n",
      "\n",
      "参数名: output.bias\n",
      "参数值: Parameter containing:\n",
      "tensor([ 0.2165, -0.2188], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.模型训练\n",
    "def train():\n",
    "    my_module = ModuleDemo()\n",
    "    print(f\"my_module: {my_module}\")\n",
    "    # 创建数据集\n",
    "    data = torch.randn(size=(5, 3))\n",
    "    print(f\"data: {data}\")\n",
    "\n",
    "    # 调用神经网络模型 -> 进行模型训练\n",
    "    output = my_module(data)  # 底层自动调用forward方法，进行前向传播\n",
    "    print(f\"output: {output}\")\n",
    "    print(f\"output shape: {output.shape}\")  # (5,2)\n",
    "    print(f\"output required_grad: {output.requires_grad}\")  # True\n",
    "\n",
    "    # 计算和查看模型参数\n",
    "    print('==============计算模型参数==============')\n",
    "    # 参1：模型对象\n",
    "    # 参2：输入数据维度（5行3列）\n",
    "    summary(my_module, input_size=(5, 3))\n",
    "    print(\"==============查看模型参数==============\")\n",
    "    # 查看模型参数\n",
    "    for name, param in my_module.named_parameters():\n",
    "        print(f\"参数名: {name}\")\n",
    "        print(f\"参数值: {param}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
